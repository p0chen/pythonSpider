### 1.浏览器配置项

```
# 浏览器类型
BROWSER: str = 'chrome'  
# 默认为Flase,如果浏览器需要使用本地登录态，则设置为True，并配置浏览器路径和缓存文件路径
IS_LOCAL_BROWSER: bool = False
# 浏览器路径
BROWSER_PATH: str = r'C:\Program Files\Google\Chrome\Application\chrome.exe'
    
#  浏览器用户缓存文件路径
USER_DIR: str = r'C:\Users\zengyanzhi\AppData\Local\Google\Chrome\User Data'
    
# 是否启用调试模式(使用本地的chrome浏览器)
DEBUG = False
# 浏览器远程调试端口()
PORT = 19789


# 是否加载图片（关闭可提升数据抓取效率）
IS_LOAD_IMAGE = True
# 是否使用无头模式
IS_HEADLESS: bool = True
# 翻页操作间隔时间(控制抓取频率，防止反爬)
TIME_INTERVAL: int = random.randint(1, 3)
```

### 2.通用爬虫

```
# 初始启动的url地址
start_url: str = ''
# 页面数据列表的定位表达式（css或xpath均支持）
data_list_loc: str = ''
```

### 3、自动翻页爬虫

```
# 自动翻页，下一页按钮的定位表达式（css或xpath均支持）
next_page_btn_loc: str = ""
# 下一页按钮距离页面底部的距离
next_button_distance: int = 200
# 数据分割的标识符(一般不用)
split_str: str = '\n'
# 抓取多少页
pages: int = 1
# 要提取的字段：{key:[v1,v2]}
# key为保存的字段名称，v1为提取的属性，v2为定位表达式（css或xpath均支持）
data_extract_loc = {
        'score': ('text', '//span[@class="real font-bold"]'),
        'name': ('text', '//span[@class="name font-bold"]'),
        'price': ('text', '//span[@class="real-price font-bold"]'),
    }
```

### 	案例：豆瓣电影TOP250

```
import asyncio
from spider.pageSpider import BasePageCrawler

class Movie2Spider(BasePageCrawler):
    # 起始url
    start_url = 'https://movie.douban.com/top250?start=0&filter='
    # 定位数据列表
    data_list_loc: str = '.grid_view li'
    # 定位 翻页按钮位置
    next_page_btn_loc: str = '.next a'
    # 页数
    pages: int = 1
    # 提取数据
    data_extract_loc: dict = {
        'title':('text','.info .hd span')
    }

async def main():
    browser = Movie2Spider()
    await browser.main()

asyncio.run(main())
```

### 4、滚动点击动态加载爬虫

```
# 动态加载更多的按钮
loader_more_loc = '//div[@class="list-btn-more"]/div'
# 加载的次数(如果加载所有数据会自动停止)
loaders = 20
```

- #### 案例：

```
class XCJDSpider(ScrollLoaderSpider):
    """携程酒店数据抓取"""
    
    # 定位数据列表
    data_list_loc = '//li[@class="list-item-target"]'
    # 动态加载更多的按钮
    loader_more_loc = '//div[@class="list-btn-more"]/div'
    # 加载的次数(如果加载所有数据会自动停止)
    loaders = 20
    
    data_extract_loc = {
        'score': ('text', '//span[@class="real font-bold"]'),
        'name': ('text', '//span[@class="name font-bold"]'),
        'price': ('text', '//span[@class="real-price font-bold"]'),
    }
    start_url = 'https://hotels.ctrip.com/hotels/list?countryId=1&city=4&checkin=2024/05/01&checkout=2024/05/03&optionId=4&optionType=City&directSearch=0&display=%E9%87%8D%E5%BA%86&crn=1&adult=1&children=0&searchBoxArg=t&travelPurpose=0&ctm_ref=ix_sb_dl&domestic=1&&highPrice=-1&barCurr=CNY&hotPoi=50%7C50%7C4197113&sort=9'
```

### 5、深度爬虫

```
# 深度页面打开的间隔时间(需要控制好，不然容易反爬)
open_page_interval: int = 3
# 深度url链接的提取规则
deep_link_url: str = ''
# 深度页面数据提取规则
deep_data_extract_loc: dict = {}
```

- #### 案例：

```
import asyncio

from spider.deepSpider import DeepPageCrawler


class DZDPSpider(DeepPageCrawler):
    DEBUG = True
    IS_LOCAL_BROWSER = True
    open_page_interval = 5
    start_url = 'https://www.dianping.com/shanghai/ch10/d1'
    data_list_loc = '//*[@id="shop-all-list"]/ul/li'
    data_extract_loc = {
        'url': ('href', '//div[@class="tit"]/a[1]'),
    }

    # ==============深度页面抓取=============
    # 深度抓取url提取规则
    deep_link_url = 'div .tit>a'
    # 深度抓取数据提取规则
    deep_data_extract_loc = {
        'addr': ('text', '#address'),
        "mobile": ('text', '//*[@id="basic-info"]/p')
    }


async def main():
    browser = DZDPSpider()
    await browser.main()


asyncio.run(main())
```

### 6、图片爬虫

```
import asyncio
from spider.mediaSpider import ImagesSpider


class JJSpider(ImagesSpider):
    # DEBUG = True
    start_url = 'https://bz.zzzmh.cn/index'
    pages = 2
    next_page_btn_loc = 'text="下一页"'
    image_start_path: str = r"https://cdn2.zzzmh.cn/wallpaper/origin/"
    image_save_path: str = r'D:\好学编程\play_w\img'

    async def opened(self):
    
    	# 打开页面之后的操作
        await asyncio.sleep(3)
        img_box = await self.page.locator('.img-box').all()
        print(f"一共有__{len(img_box)}张图片")
        await self.page.wait_for_timeout(self.TIME_INTERVAL)
        await self.page.click('.img-box>div>div')
        for i in range(0, len(img_box)):
            await self.page.wait_for_timeout(self.TIME_INTERVAL)
            await self.page.click('//button[@title="下一张"]')

        await asyncio.sleep(3)
        await self.page.click('.close-span')


async def main():
    browser = JJSpider()
    await browser.main()


asyncio.run(main())
```

### 7、视频下载

```
import asyncio
import random
import time

from spider.mediaSpider import VideoSpider


class DouYinSpider(VideoSpider):
    """抖音up账号视频爬虫"""

    DEBUG = True
    IS_LOCAL_BROWSER = True
    # 要抓取的抖音up主的首页地址
    start_url: str = 'https://www.douyin.com/user/MS4wLjABAAAAOlZ8ngnt417GKBbFysKt2Q8ERj84-Wb9xypbB8_hmIc?vid=7369137414838684954'
    # 视频保存路径
    video_save_path: str = r'D:\好学编程\play_w\木瓜电影'
    # 从url中提取文件名的规则
    file_name_pattern: str = r'.com/.+?/(.+?)/video'
    # 音频文件标签
    audio_tag: str = 'media-audio-und-mp4a'
    # 视频文件标签
    video_tag: str = 'media-video-hvc1'

    async def opened(self):
        # 获取所有的url
        a_list = await self.page.locator('//ul[@class="e6wsjNLL bGEvyQfj"]//a').all()
        print("up主的视频数量：", len(a_list))
        for i in range(len(a_list)):
            if i == 0:
                await a_list[i].click()
            time.sleep(random.randint(3, 8))
            await self.page.mouse.wheel(0, 100)


async def main():
    browser = DouYinSpider()
    await browser.main()


asyncio.run(main())
```